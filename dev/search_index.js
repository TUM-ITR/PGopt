var documenterSearchIndex = {"docs":
[{"location":"list_of_functions/#List-of-fuctions","page":"List of fuctions","title":"List of fuctions","text":"","category":"section"},{"location":"list_of_functions/","page":"List of fuctions","title":"List of fuctions","text":"particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi::Function, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R; x_prim=nothing)\nsolve_PG_OCP(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, active_constraints=nothing, opts=nothing, print_progress=true)\nsolve_PG_OCP_greedy_guarantees(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag, β; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, opts=nothing, print_progress=true)\ntest_prediction(PG_samples::Vector{PG_sample}, phi::Function, g, R, k_n, u_test, y_test)\nplot_predictions(y_pred, y_test; plot_percentiles=false, y_min=nothing, y_max=nothing)\nplot_autocorrelation(PG_samples::Vector{PG_sample}; max_lag=0)\nepsilon(s::Int64, K::Int64, β::Float64)\nMNIW_sample(Phi, Psi, Sigma, V, Lambda_Q, ell_Q, T)\nsystematic_resampling(W, N)","category":"page"},{"location":"list_of_functions/#PGopt.particle_Gibbs-Tuple{Any, Any, Any, Any, Any, Any, Function, Vararg{Any, 8}}","page":"List of fuctions","title":"PGopt.particle_Gibbs","text":"particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi::Function, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R; x_prim=nothing)\n\nRun particle Gibbs sampler with ancestor sampling to obtain samples A Q x_T-1^1K from the joint parameter and state posterior distribution p(A Q x_T-1 mid mathbbD=u_T-1 y_T-1).\n\nArguments\n\nu_training: training input trajectory\ny_training: training output trajectory\nK: number of models/scenarios to be sampled\nK_b: length of the burn in period\nk_d: number of models/scenarios to be skipped to decrease correlation (thinning)\nN: number of particles\nphi: basis functions\nLambda_Q: scale matrix of IW prior on Q\nell_Q: degrees of freedom of IW prior on Q\nQ_init: initial value of Q\nV: left covariance matrix of MN prior on A\nA_init: initial value of A\nx_init_dist: distribution of the initial state\ng: observation function\nR: variance of zero-mean Gaussian measurement noise\nx_prim: prespecified trajectory for the first iteration\n\nThis function is based on the papers\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nF. Lindsten, T. B. Schön, and M. Jordan, “Ancestor sampling for particle Gibbs,” Advances in Neural Information Processing Systems, vol. 25, 2012.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.solve_PG_OCP-Tuple{Vector{PG_sample}, Function, Vararg{Any, 7}}","page":"List of fuctions","title":"PGopt.solve_PG_OCP","text":"solve_PG_OCP(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, active_constraints=nothing, opts=nothing, print_progress=true)\n\nSolve the optimal control problem of the following form:\n\nmin sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t\n\nsubject to:\n\nbeginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned\n\nNote that the output constraints imply the measurement function y_t^k = x_t 1n_y^k. Further note that the states of the individual models (x^1k) are combined in the vector x_vec of dimension K * n_x.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\nR: variance of zero-mean Gaussian measurement noise - only used if e_vec is not passed\nH: horizon of the OCP\nu_min: array of dimension 1 x 1 or 1 x H containing the minimum control input for all timesteps\nu_max: array of dimension 1 x 1 or 1 x H containing the maximum control input for all timesteps\ny_min: array of dimension 1 x 1 or 1 x H containing the minimum system output for all timesteps\ny_max: array of dimension 1 x 1 or 1 x H containing the maximum system output for all timesteps\nR_cost_diag: parameter of the diagonal quadratic cost function\nx_vec_0: vector with K n_x elements containing the initial state of all models - if not provided, the initial states are sampled based on the PGS samples\nv_vec: array of dimension n_x x H x K that contains the process noise for all models and all timesteps - if not provided, the noise is sampled based on the PGS samples\ne_vec: array of dimension n_y x H x K that contains the measurement noise for all models and all timesteps - if not provided, the noise is sampled based on the provided R\nu_init: initial guess for the optimal trajectory\nK_pre_solve: if K_pre_solve > 0, an initial guess for the optimal trajectory is obtained by solving the OCP with only K_pre_solve < K models\nactive_constraints: vector containing the indices of the models, for which the output constraints are active - if not provided, the output constraints are considered for all models\nopts: SolverOptions struct containing options of the solver\nprint_progress: if set to true, the progress is printed\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.solve_PG_OCP_greedy_guarantees-Tuple{Vector{PG_sample}, Function, Vararg{Any, 8}}","page":"List of fuctions","title":"PGopt.solve_PG_OCP_greedy_guarantees","text":"solve_PG_OCP_greedy_guarantees(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag, β; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, opts=nothing, print_progress=true)\n\nSolve the following optimal control problem and determine a support sub-sample with cardinality s via a greedy constraint removal.  Based on the cardinality s, a bound on the probability that the incurred cost exceeds the worst-case cost or that the constraints are violated when the input trajectory u_0H is applied to the unknown system is calculated (i.e., 1-epsilon is determined).\n\nmin sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t\n\nsubject to:\n\nbeginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned\n\nNote that the output constraints imply the measurement function y_t^k = x_t 1n_y^k. Further note that the states of the individual models (x^1k) are combined in the vector x_vec of dimension K * n_x.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\nR: variance of zero-mean Gaussian measurement noise - only used if e_vec is not passed\nH: horizon of the OCP\nu_min: array of dimension 1 x 1 or 1 x H containing the minimum control input for all timesteps\nu_max: array of dimension 1 x 1 or 1 x H containing the maximum control input for all timesteps\ny_min: array of dimension 1 x 1 or 1 x H containing the minimum system output for all timesteps\ny_max: array of dimension 1 x 1 or 1 x H containing the maximum system output for all timesteps\nR_cost_diag: parameter of the diagonal quadratic cost function\nβ: confidence parameter\nx_vec_0: vector with K*n_x elements containing the initial state of all models - if not provided, the initial states are sampled based on the PGS samples\nv_vec: array of dimension n_x x H x K that contains the process noise for all models and all timesteps - if not provided, the noise is sampled based on the PGS samples\ne_vec: array of dimension n_y x H x K that contains the measurement noise for all models and all timesteps - if not provided, the noise is sampled based on the provided R\nu_init: initial guess for the optimal trajectory\nK_pre_solve: if K_pre_solve > 0, an initial guess for the optimal trajectory is obtained by solving the OCP with only K_pre_solve < K models\nopts: SolverOptions struct containing options of the solver\nprint_progress: if set to true, the progress is printed\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.test_prediction-Tuple{Vector{PG_sample}, Function, Vararg{Any, 5}}","page":"List of fuctions","title":"PGopt.test_prediction","text":"test_prediction(PG_samples::Vector{PG_sample}, phi::Function, g, R, k_n, u_test, y_test)\n\nSimulate the PGS samples forward in time and compare the predictions to the test data.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\ng: observation function\nR: variance of zero-mean Gaussian measurement noise\nk_n: each model is simulated k_n times\nu_test: test input\ny_test: test output\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.plot_predictions-Tuple{Any, Any}","page":"List of fuctions","title":"PGopt.plot_predictions","text":"plot_predictions(y_pred, y_test; plot_percentiles=false, y_min=nothing, y_max=nothing)\n\nPlot the predictions and the test data.\n\nArguments\n\ny_pred: matrix containing the output predictions\ny_test: test output trajectory\nplot_percentiles: if set to true, percentiles are plotted\ny_min: min output to be plotted as constraint\ny_max: max output to be plotted as constraint\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.plot_autocorrelation-Tuple{Vector{PG_sample}}","page":"List of fuctions","title":"PGopt.plot_autocorrelation","text":"plot_autocorrelation(PG_samples::Vector{PG_sample}; max_lag=0)\n\nPlot the autocorrelation function (ACF) of the PG samples. This might be helpful when adjusting the thinning parameter k_d.\n\nArguments\n\nPG_samples: PG samples\nmax_lag: maximum lag at which to calculate the ACF\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.epsilon-Tuple{Int64, Int64, Float64}","page":"List of fuctions","title":"PGopt.epsilon","text":"epsilon(s::Int64, K::Int64, β::Float64)\n\nDetermine the parameter epsilon. 1-epsilon corresponds to a bound on the probability that the incurred cost exceeds the worst-case cost or that the constraints are violated when the input trajectory u_0H is applied to the unknown system. epsilon is the unique solution over the interval (01) of the polynomial equation in the v variable:\n\nbinomKs(1-v)^K-s-fracbetaKsum_m=s^K-1binomms(1-v)^m-s=0.\n\nArguments\n\ns: cardinality of the support sub-sample \nK: number of scenarios\nβ: confidence parameter\n\nThis function is based on the paper\n\nS. Garatti and M. C. Campi, “Risk and complexity in scenario optimization,” Mathematical Programming, vol. 191, no. 1, pp. 243–279, 2022.\n\nand the code provided in the appendix.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.MNIW_sample-NTuple{7, Any}","page":"List of fuctions","title":"PGopt.MNIW_sample","text":"MNIW_sample(Phi, Psi, Sigma, V, Lambda_Q, ell_Q, T)\n\nSample new model parameters A Q from the conditional distribution p(A Q  x_T-1), which is a matrix normal inverse Wishart (MNIW) distribution.\n\nArguments\n\nPhi: statistic; see paper below for definition\nPsi: statistic; see paper below for definition\nSigma: statistic; see paper below for definition\nV: left covariance matrix of MN prior on A\nLambda_Q: scale matrix of IW prior on Q\nell_Q: degrees of freedom of IW prior on Q\nT: length of the training trajectory\n\nThis function is based on the paper\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.systematic_resampling-Tuple{Any, Any}","page":"List of fuctions","title":"PGopt.systematic_resampling","text":"systematic_resampling(W, N)\n\nSample N indices according to the weights W.\n\nArguments\n\nW: vector containing the weights of the particles\nN: number of indices to be sampled\n\nThis function is based on the paper\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"If you found this software useful for your research, consider citing us.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"@article{PMCMC_OCP_arXiv_2023,\n      title={Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States},\n      author={Lefringhausen, Robert and Srithasan, Supitsana and Lederer, Armin and Hirche, Sandra},\n      journal={arXiv preprint arXiv:2303.17963},\n      year={2023}\n}","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"PGopt is a software for determining optimal input trajectories with probabilistic performance and constraint satisfaction guarantees for unknown systems with latent states based on input-output measurements. In order to quantify uncertainties, which is crucial for deriving formal guarantees, a Bayesian approach is employed and a prior over the unknown dynamics and the system trajectory is formulated in state-space representation. Since for practical applicability, the prior must be updated based on input-output measurements, but the corresponding posterior distribution is analytically intractable, particle Gibbs (PG) sampling is utilized to draw samples from this distribution. Based on these samples, a scenario optimal control problem (OCP) is formulated and probabilistic performance and constraint satisfaction guarantees are inferred via a greedy constraint removal.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The approach is explained in the paper \"Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States\", available as a preprint on arXiv.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"PGopt can be installed using the Julia package manager. Inside the Julia REPL, type ] to enter the Pkg REPL mode then run","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add https://github.com/TUM-ITR/PGopt:Julia","category":"page"},{"location":"#Examples","page":"Introduction","title":"Examples","text":"","category":"section"}]
}
