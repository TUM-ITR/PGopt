var documenterSearchIndex = {"docs":
[{"location":"examples/PG_OCP_known_basis_functions/#Optimal-control-with-known-basis-functions","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"","category":"section"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"This example reproduces the results of the optimal control approach with known basis functions (Figure 2) given in Section V-B of the paper.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"(Image: autocorrelation)","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"### Support sub-sample found\nCardinality of the support sub-sample (s): 7\nMax. constraint violation probability (1-epsilon): 10.22 %","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"First, the algorithm and simulation parameters are defined, and training data is generated. Then, by calling the function particle_Gibbs(), samples are drawn from the posterior distribution using particle Gibbs sampling. These samples are then passed to the function solve_PG_OCP_greedy_guarantees(), which solves the scenario OCP and inferres probabilistic constraint satisfaction guarantees by greedily removing constraints and solving the corresponding reduced OCP.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"For the results in Table II of the paper, this script is repeated with seeds 1:100.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"A Julia script that contains all the steps described here can be found in the folder PGopt/Julia/examples.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"The runtime of the script is about 2 hours on a standard laptop.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/#Define-prior-and-parameters","page":"Optimal control with known basis functions","title":"Define prior and parameters","text":"","category":"section"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"First, load packages and initialize.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"using PGopt\nusing LinearAlgebra\nusing Random\nusing Distributions\nusing Printf\nusing Plots\n\n# Specify seed (for reproducible results).\nRandom.seed!(82)\n\n# Time PGS algorithm.\nsampling_timer = time()","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Then, specify the parameters of the algorithm.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"# Learning parameters\nK = 200 # number of PG samples\nk_d = 50 # number of samples to be skipped to decrease correlation (thinning)\nK_b = 1000 # length of burn-in period\nN = 30 # number of particles of the particle filter\n\n# Number of states, etc.\nn_x = 2 # number of states\nn_u = 1 # number of control inputs\nn_y = 1 # number of outputs","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Define the basis functions. The basis functions are assumed to be known in this example. Make sure that phi(x u) is defined in vectorized form, i.e., phi(zeros(n_x, N), zeros(n_u, N)) should return a matrix of dimension (n_phi, N).","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"n_phi = 5 # number of basis functions\nphi(x, u) = [0.1 * x[1, :] 0.1 * x[2, :] u[1, :] 0.01 * cos.(3 * x[1, :]) .* x[2, :] 0.1 * sin.(2 * x[2, :]) .* u[1, :]]' # basis functions","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Select the parameters of the inverse Wishart prior for Q.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"ell_Q = 10 # degrees of freedom\nLambda_Q = 100 * I(n_x) # scale matrix","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Select the parameters of the matrix normal prior (with mean matrix 0, right covariance matrix Q (see above), and left covariance matrix V) for A.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"V = Diagonal(10 * ones(n_phi)) # left covariance matrix","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Provide an initial guess for the parameters.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Q_init = Lambda_Q # initial Q\nA_init = zeros(n_x, n_phi) # initial A","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Choose the distribution of the initial state. Here, a normally distributed initial state is assumed.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"x_init_mean = [2, 2] # mean\nx_init_var = 1 * I # variance\nx_init_dist = MvNormal(x_init_mean, x_init_var)","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Define the measurement model. It is assumed to be known (without loss of generality). Make sure that g(x u) is defined in vectorized form, i.e., g(zeros(n_x, N), zeros(n_u, N)) should return a matrix of dimension (n_y, N).","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"g(x, u) = [1 0] * x # observation function\nR = 0.1 # variance of zero-mean Gaussian measurement noise","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/#Generate-data","page":"Optimal control with known basis functions","title":"Generate data","text":"","category":"section"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Generate training data.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"# Parameters for data generation\nT = 2000 # number of steps for training\nT_test = 500  # number of steps used for testing (via forward simulation - see below)\nT_all = T + T_test\n\n# Unknown system\nf_true(x, u) = [0.8 * x[1, :] - 0.5 * x[2, :] + 0.1 * cos.(3 * x[1, :]) .* x[2, :]; 0.4 * x[1, :] + 0.5 * x[2,:] + (ones(size(x, 2)) + 0.3 * sin.(2 * x[2, :])) .* u[1, :]] # true state transition function\nQ_true = [0.03 -0.004; -0.004 0.01] # true process noise variance\nmvn_v_true = MvNormal(zeros(n_x), Q_true) # true process noise distribution\ng_true = g # true measurement function\nR_true = R # true measurement noise variance\nmvn_e_true = MvNormal(zeros(n_y), R_true) # true measurement noise distribution\n\n# Input trajectory used to generate training and test data\nmvn_u_training = Normal(0, 3) # training input distribution\nu_training = rand(mvn_u_training, T) # training inputs\nu_test = 3 * sin.(2 * pi * (1 / T_test) * (Array(1:T_test) .- 1)) # test inputs\nu = reshape([u_training; u_test], 1, T_all) # training + test inputs\n\n# Generate data by forward simulation.\nx = Array{Float64}(undef, n_x, T_all + 1) # true latent state trajectory\nx[:, 1] = rand(x_init_dist, 1) # random initial state\ny = Array{Float64}(undef, n_y, T_all) # output trajectory (measured)\nfor t in 1:T_all\n    x[:, t+1] = f_true(x[:, t], u[:, t]) + rand(mvn_v_true, 1)\n    y[:, t] = g_true(x[:, t], u[:, t]) + rand(mvn_e_true, 1)\nend\n\n# Split data into training and test data.\nu_training = u[:, 1:T]\nx_training = x[:, 1:T+1]\ny_training = y[:, 1:T]\n\nu_test = u[:, T+1:end]\nx_test = x[:, T+1:end]\ny_test = y[:, T+1:end]","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/#Infer-model","page":"Optimal control with known basis functions","title":"Infer model","text":"","category":"section"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Run the particle Gibbs sampler to jointly estimate the model parameters and the latent state trajectory.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"PG_samples = particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R)\n\ntime_sampling = time() - sampling_timer","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/#Define-and-solve-optimal-control-problem","page":"Optimal control with known basis functions","title":"Define and solve optimal control problem","text":"","category":"section"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Afterward, define the optimal control problem of the form","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"min sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"subject to:","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"beginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"(Note that the output constraints imply the measurement function y_t^k = x_t 1n_y^k.)","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"# Horizon\nH = 41\n\n# Define constraints for u and y.\nu_max = [5] # max control input\nu_min = [-5] # min control input\ny_max = reshape(fill(Inf, H), (1, H)) # max system output\ny_min = reshape([-fill(Inf, 20); 2 * ones(6); -fill(Inf, 15)], (1, H)) # min system output\n\nR_cost_diag = [2] # diagonal of R_cost","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Select the confidence parameter for the theoretical guarantees.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"β = 0.01","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Solve the optimal control problem and determine a support sub-sample with cardinality s via a greedy constraint removal.  Based on the cardinality s, a bound on the probability that the incurred cost exceeds the worst-case cost or that the constraints are violated when the input trajectory u_0H is applied to the unknown system is calculated (i.e., 1-epsilon is determined).","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"x_opt, u_opt, y_opt, J_opt, s, epsilon_prob, epsilon_perc, time_first_solve, time_guarantees, num_failed_optimizations = solve_PG_OCP_greedy_guarantees(PG_samples, phi, R, H, u_min, u_max, y_min, y_max, R_cost_diag, β; K_pre_solve=20)","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"Finally, apply the input trajectory to the actual system and plot the output trajectories.","category":"page"},{"location":"examples/PG_OCP_known_basis_functions/","page":"Optimal control with known basis functions","title":"Optimal control with known basis functions","text":"# Apply input trajectory to the actual system.\ny_sys = Array{Float64}(undef, n_y, H)\nx_sys = Array{Float64}(undef, n_x, H)\nx_sys[:, 1] = x_training[:, end]\nu_sys = [u_opt 0]\nfor t in 1:H\n    if t >= 2\n        x_sys[:, t] = f_true(x_sys[:, t-1], u_sys[:, t-1]) + rand(mvn_v_true, 1)\n    end\n    y_sys[:, t] = g_true(x_sys[:, t], u_sys[:, t]) + rand(mvn_e_true, 1)\nend\n\n# Plot predictions.\nplot_predictions(y_opt, y_sys; plot_percentiles=false, y_min=y_min, y_max=y_max)","category":"page"},{"location":"examples/autocorrelation/#Autocorrelation","page":"Autocorrelation","title":"Autocorrelation","text":"","category":"section"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"This example reproduces the normalized auto-correlation function plot (Figure 1) in Section V-B of the paper. ","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"(Image: autocorrelation)","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Assuming knowledge of the basis functions, samples are drawn from the posterior distribution over model parameters and latent state trajectories using the function particle_Gibbs() without thinning. Afterward, the autocorrelation is plotted using the function plot_autocorrelation().","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"A Julia script that contains all the steps described here can be found in the folder PGopt/Julia/examples.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"The runtime of the script is about 10 minutes on a standard laptop.","category":"page"},{"location":"examples/autocorrelation/#Define-prior-and-parameters","page":"Autocorrelation","title":"Define prior and parameters","text":"","category":"section"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"First, load packages and initialize.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"using PGopt\nusing LinearAlgebra\nusing Random\nusing Distributions\nusing Printf\nusing Plots\n\n# Specify seed (for reproducible results).\nRandom.seed!(82)\n\n# Time PGS algorithm.\nsampling_timer = time()","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Then, specify the parameters of the algorithm.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"# Learning parameters\nK = 10000 # number of PG samples\nk_d = 0 # number of samples to be skipped to decrease correlation (thinning)\nK_b = 1000 # length of burn-in period\nN = 30 # number of particles of the particle filter\n\n# Number of states, etc.\nn_x = 2 # number of states\nn_u = 1 # number of control inputs\nn_y = 1 # number of outputs","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Define the basis functions. The basis functions are assumed to be known in this example. Make sure that phi(x,u) is defined in vectorized form, i.e., phi(zeros(nx, N), zeros(nu, N)) should return a matrix of dimension (n_phi, N).","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"n_phi = 5 # number of basis functions\nphi(x, u) = [0.1 * x[1, :] 0.1 * x[2, :] u[1, :] 0.01 * cos.(3 * x[1, :]) .* x[2, :] 0.1 * sin.(2 * x[2, :]) .* u[1, :]]' # basis functions","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Select the parameters of the inverse Wishart prior for Q.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"ell_Q = 10 # degrees of freedom\nLambda_Q = 100 * I(n_x) # scale matrix","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Select the parameters of the matrix normal prior (with mean matrix = 0, right covariance matrix = Q (see above), and left covariance matrix = V) for Q.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"V = Diagonal(10 * ones(n_phi)) # left covariance matrix","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Provide an initial guess for the parameters.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Q_init = Lambda_Q # initial Q\nA_init = zeros(n_x, n_phi) # initial A","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Choose the distribution of the initial state. Here, a normally distributed initial state is assumed.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"x_init_mean = [2, 2] # mean\nx_init_var = 1 * I # variance\nx_init_dist = MvNormal(x_init_mean, x_init_var)","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Define the measurement model. It is assumed to be known (without loss of generality). Make sure that g(x, u) is defined in vectorized form, i.e., g(zeros(nx, N), zeros(nu, N)) should return a matrix of dimension (n_y, N).","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"g(x, u) = [1 0] * x # observation function\nR = 0.1 # variance of zero-mean Gaussian measurement noise","category":"page"},{"location":"examples/autocorrelation/#Generate-data","page":"Autocorrelation","title":"Generate data","text":"","category":"section"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Generate training data.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"# Parameters for data generation\nT = 2000 # number of steps for training\nT_test = 500  # number of steps used for testing (via forward simulation - see below)\nT_all = T + T_test\n\n# Unknown system\nf_true(x, u) = [0.8 * x[1, :] - 0.5 * x[2, :] + 0.1 * cos.(3 * x[1, :]) .* x[2, :]; 0.4 * x[1, :] + 0.5 * x[2,:] + (ones(size(x, 2)) + 0.3 * sin.(2 * x[2, :])) .* u[1, :]] # true state transition function\nQ_true = [0.03 -0.004; -0.004 0.01] # true process noise variance\nmvn_v_true = MvNormal(zeros(n_x), Q_true) # true process noise distribution\ng_true = g # true measurement function\nR_true = R # true measurement noise variance\nmvn_e_true = MvNormal(zeros(n_y), R_true) # true measurement noise distribution\n\n# Input trajectory used to generate training and test data\nmvn_u_training = Normal(0, 3) # training input distribution\nu_training = rand(mvn_u_training, T) # training inputs\nu_test = 3 * sin.(2 * pi * (1 / T_test) * (Array(1:T_test) .- 1)) # test inputs\nu = reshape([u_training; u_test], 1, T_all) # training + test inputs\n\n# Generate data by forward simulation.\nx = Array{Float64}(undef, n_x, T_all + 1) # true latent state trajectory\nx[:, 1] = rand(x_init_dist, 1) # random initial state\ny = Array{Float64}(undef, n_y, T_all) # output trajectory (measured)\nfor t in 1:T_all\n    x[:, t+1] = f_true(x[:, t], u[:, t]) + rand(mvn_v_true, 1)\n    y[:, t] = g_true(x[:, t], u[:, t]) + rand(mvn_e_true, 1)\nend\n\n# Split data into training and test data.\nu_training = u[:, 1:T]\nx_training = x[:, 1:T+1]\ny_training = y[:, 1:T]\n\nu_test = u[:, T+1:end]\nx_test = x[:, T+1:end]\ny_test = y[:, T+1:end]\n","category":"page"},{"location":"examples/autocorrelation/#Infer-model","page":"Autocorrelation","title":"Infer model","text":"","category":"section"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Run the particle Gibbs sampler to jointly estimate the model parameters and the latent state trajectory.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"PG_samples = particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R)\n\ntime_sampling = time() - sampling_timer","category":"page"},{"location":"examples/autocorrelation/#Plot-autocorrelation","page":"Autocorrelation","title":"Plot autocorrelation","text":"","category":"section"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"Finally, plot the autocorrelation of the obtained samples.","category":"page"},{"location":"examples/autocorrelation/","page":"Autocorrelation","title":"Autocorrelation","text":"plot_autocorrelation(PG_samples; max_lag=100)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Optimal-control-with-generic-basis-functions","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"This example reproduces the results of the optimal control approach with generic basis functions (Figure 3) given in Section V-C of the paper.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"(Image: autocorrelation)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"The method presented in the paper \"A flexible state–space model for learning nonlinear dynamical systems\" is utilized to systematically derive basis functions and priors for the parameters based on a reduced-rank GP approximation. Afterward, by calling the function particle_Gibbs(), samples are drawn from the posterior distribution using particle Gibbs sampling. These samples are then passed to the function solve_PG_OCP(), which solves the scenario OCP.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"For the results in Table IV of the paper, this script is repeated with seeds 1:100.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"A Julia script that contains all the steps described here can be found in the folder PGopt/Julia/examples.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"The runtime of the script is about 2 hours on a standard laptop. Using an improved function phi() can reduce the runtime to about 50 minutes, but the results change slightly due to numerical reasons. Further explanations are given below.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Define-parameters","page":"Optimal control with generic basis functions","title":"Define parameters","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"First, load packages and initialize.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"using PGopt\nusing LinearAlgebra\nusing Random\nusing Distributions\nusing Printf\nusing Plots\n\n# Specify seed (for reproducible results).\nRandom.seed!(82)\n\n# Time PGS algorithm.\nsampling_timer = time()","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Then, specify the parameters of the algorithm.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"# Learning parameters\nK = 100 # number of PG samples\nk_d = 50 # number of samples to be skipped to decrease correlation (thinning)\nK_b = 1000 # length of burn-in period\nN = 30 # number of particles of the particle filter\n\n# Number of states, etc.\nn_x = 2 # number of states\nn_u = 1 # number of control inputs\nn_y = 1 # number of outputs","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Define-basis-functions","page":"Optimal control with generic basis functions","title":"Define basis functions","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Then, generate generic basis functions and priors based on a reduced-rank GP approximation. The approach is described in the paper \"A flexible state–space model for learning nonlinear dynamical systems\". The equation numbers given in the following refer to this paper.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"n_phi_x = [5 5] # number of basis functions for each state\nn_phi_u = 5 # number of basis functions for the control input\nn_phi_dims = [n_phi_u n_phi_x] # array containing the number of basis functions for each input dimension\nn_phi = prod(n_phi_dims) # total number of basis functions\nl_x = 20\nL_x = [l_x l_x] # interval lengths for x\nL_u = 10 # interval length for u\nL = zeros(1, 1, n_z) # array containing the interval lengths\nL[1, 1, :] = [L_u L_x]\n\n# Hyperparameters of the squared exponential kernel\nl = [2] # length scale\nsf = 100 # scale factor\n\n# Initialize.\nj_vec = zeros(n_phi, 1, n_z) # contains all possible vectors j; j_vec[i, 1, :] corresponds to the vector j in eq. (5) for basis function i\nlambda = zeros(n_phi, n_z) # lambda[i, :] corresponds to the vector λ in eq. (9) (right-hand side) for basis function i","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"In the following, all possible vectors j are constructed (i.e., j_vec). The possible combinations correspond to the Cartesian product [1 : n_basis[1]] x ... x [1 : n_basis[end]].","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"cart_prod_sets = Array{Any}(undef, n_z) # array of arrays; cart_prod_sets[i] corresponds to the i-th set to be considered for the Cartesian product, i.e., [1 : n_basis[i]].\nfor i = 1:n_z\n    cart_prod_sets[i] = Array(1:n_phi_dims[i])\nend\n\nsubscript_values = Array{Int64}(undef, n_z) # contains the equivalent subscript values corresponding to a given single index i\nvariants = [1; cumprod(n_phi_dims[1:end-1])] # required to convert the single index i to the equivalent subscript value\n\n# Construct Cartesian product and calculate spectral densities.\nfor i in 1:n_phi\n    # Convert the single index i to the equivalent subscript values.\n    remaining = i - 1\n    for j in n_z:-1:1\n        subscript_values[j] = floor(remaining / variants[j]) + 1\n        remaining = mod(remaining, variants[j])\n    end\n\n    # Fill j_vec with the values belonging to the respective subscript indices.\n    for j in 1:n_z\n        j_vec[i, 1, j] = cart_prod_sets[j][subscript_values[j]]\n    end\n\n    # Calculate the eigenvalue of the Laplace operator corresponding to the vector j_vec[i, 1, :] - see eq. (9) (right-hand side).\n    lambda[i, :] = (pi .* j_vec[i, 1, :] ./ (2 * dropdims(L, dims=tuple(findall(size(L) .== 1)...)))) .^ 2\nend\n\n# Reverse j_vec.\nj_vec = reverse(j_vec, dims=3)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Then, define basis functions phi. This function evaluates phi_1  n_x+n_u according to eq. (5). There are two implementations of the phi function.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Original-implementation","page":"Optimal control with generic basis functions","title":"Original implementation","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"This implementation is the original implementation, which is slow but exactly reproduces the results given in the paper.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"function phi_sampling(x, u)\n    # Initialize.\n    z = vcat(u, x) # augmented state\n    phi = Array{Float64}(undef, n_phi, size(z, 2))\n\n    Threads.@threads for i in axes(z, 2)\n        phi_temp = ones(n_phi)\n        for k in axes(z, 1)\n            phi_temp .= phi_temp .* ((1 ./ (sqrt.(L[:, :, k]))) .* sin.((pi .* j_vec[:, :, k] .* (z[k, i] .+ L[:, :, k])) ./ (2 .* L[:, :, k])))\n        end\n        phi[:, i] .= phi_temp\n    end\n    return phi\nend","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Efficient-implementation","page":"Optimal control with generic basis functions","title":"Efficient implementation","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"After the paper's publication, a much more efficient implementation was found, which, for numerical reasons, produces slightly different results (differences in the range 1cdot 10^-16 when called once).  However, these minimal deviations lead to noticeably different results since the function phi is called recursively T cdot N cdot K_mathrmtotal times (= very often).  This is the improved implementation, which is significantly faster but yields slightly different results.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"# Precompute.\nL_sqrt_inv = 1 ./ sqrt.(L)\npi_j_over_2L = pi .* j_vec ./ (2 .* L)\nfunction phi_sampling(x, u)\n    # Initialize.\n    z = vcat(u, x) # augmented state\n    phi = ones(n_phi, size(z, 2))\n\n    for k in axes(z, 1)\n        phi .= phi .* (L_sqrt_inv[:, :, k] .* sin.(pi_j_over_2L[:, :, k] * (z[k, :] .+ L[:, :, k])'))\n    end\n\n    return phi\nend","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Define-prior","page":"Optimal control with generic basis functions","title":"Define prior","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Select the parameters of the inverse Wishart prior for Q.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"ell_Q = 10 # degrees of freedom\nLambda_Q = 100 * I(n_x) # scale matrix","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Determine the parameters of the matrix normal prior (with mean matrix 0, right covariance matrix Q (see above), and left covariance matrix V) for A. V is derived from the GP approximation according to eq. (8b), (11a), and (9).","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"V_diagonal = Array{Float64}(undef, size(lambda, 1)) # diagonal of V\nfor i in axes(lambda, 1)\n    V_diagonal[i] = sf^2 * sqrt(opnorm(2 * pi * Diagonal(repeat(l, trunc(Int, n_z / size(l, 1))) .^ 2))) * exp.(-(pi^2 * transpose(sqrt.(lambda[i, :])) * Diagonal(repeat(l, trunc(Int, n_z / size(l, 1))) .^ 2) * sqrt.(lambda[i, :])) / 2)\nend\nV = Diagonal(V_diagonal)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Provide an initial guess for the parameters.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Q_init = Lambda_Q # initial Q\nA_init = zeros(n_x, n_phi) # initial A","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Choose the distribution of the initial state. Here, a normally distributed initial state is assumed.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"x_init_mean = [2, 2] # mean\nx_init_var = 1 * I # variance\nx_init_dist = MvNormal(x_init_mean, x_init_var)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Define the measurement model. It is assumed to be known (without loss of generality). Make sure that g(x u) is defined in vectorized form, i.e., g(zeros(n_x, N), zeros(n_u, N)) should return a matrix of dimension (n_y, N).","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"g(x, u) = [1 0] * x # observation function\nR = 0.1 # variance of zero-mean Gaussian measurement noise","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Generate-data","page":"Optimal control with generic basis functions","title":"Generate data","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Generate training data.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"# Parameters for data generation\nT = 2000 # number of steps for training\nT_test = 500  # number of steps used for testing (via forward simulation - see below)\nT_all = T + T_test\n\n# Unknown system\nf_true(x, u) = [0.8 * x[1, :] - 0.5 * x[2, :] + 0.1 * cos.(3 * x[1, :]) .* x[2, :]; 0.4 * x[1, :] + 0.5 * x[2,:] + (ones(size(x, 2)) + 0.3 * sin.(2 * x[2, :])) .* u[1, :]] # true state transition function\nQ_true = [0.03 -0.004; -0.004 0.01] # true process noise variance\nmvn_v_true = MvNormal(zeros(n_x), Q_true) # true process noise distribution\ng_true = g # true measurement function\nR_true = R # true measurement noise variance\nmvn_e_true = MvNormal(zeros(n_y), R_true) # true measurement noise distribution\n\n# Input trajectory used to generate training and test data\nmvn_u_training = Normal(0, 3) # training input distribution\nu_training = rand(mvn_u_training, T) # training inputs\nu_test = 3 * sin.(2 * pi * (1 / T_test) * (Array(1:T_test) .- 1)) # test inputs\nu = reshape([u_training; u_test], 1, T_all) # training + test inputs\n\n# Generate data by forward simulation.\nx = Array{Float64}(undef, n_x, T_all + 1) # true latent state trajectory\nx[:, 1] = rand(x_init_dist, 1) # random initial state\ny = Array{Float64}(undef, n_y, T_all) # output trajectory (measured)\nfor t in 1:T_all\n    x[:, t+1] = f_true(x[:, t], u[:, t]) + rand(mvn_v_true, 1)\n    y[:, t] = g_true(x[:, t], u[:, t]) + rand(mvn_e_true, 1)\nend\n\n# Split data into training and test data.\nu_training = u[:, 1:T]\nx_training = x[:, 1:T+1]\ny_training = y[:, 1:T]\n\nu_test = u[:, T+1:end]\nx_test = x[:, T+1:end]\ny_test = y[:, T+1:end]","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Infer-model","page":"Optimal control with generic basis functions","title":"Infer model","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Run the particle Gibbs sampler to jointly estimate the model parameters and the latent state trajectory.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"PG_samples = particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R)\n\ntime_sampling = time() - sampling_timer","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/#Define-and-solve-optimal-control-problem","page":"Optimal control with generic basis functions","title":"Define and solve optimal control problem","text":"","category":"section"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Afterward, define the optimal control problem of the form","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"min sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"subject to:","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"beginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"(Note that the output constraints imply the measurement function y_t^k = x_t 1n_y^k.)","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"# Horizon\nH = 41\n\n# Define constraints for u and y.\nu_max = [5] # max control input\nu_min = [-5] # min control input\ny_max = reshape(fill(Inf, H), (1, H)) # max system output\ny_min = reshape([-fill(Inf, 20); 2 * ones(6); -fill(Inf, 15)], (1, H)) # min system output\n\nR_cost_diag = [2] # diagonal of R_cost","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Solve the optimal control problem. In this case, no formal guarantees for the constraint satisfaction can be derived since Assumption 1 is not satisfied as the employed basis functions cannot represent the actual dynamics with arbitrary precision.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"x_opt, u_opt, y_opt, J_opt, penalty_max = solve_PG_OCP(PG_samples, phi_opt, R, H, u_min, u_max, y_min, y_max, R_cost_diag; K_pre_solve=20, opts=opts)[[1, 2, 3, 4, 8]]","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"Finally, apply the input trajectory to the actual system and plot the output trajectories.","category":"page"},{"location":"examples/PG_OCP_generic_basis_functions/","page":"Optimal control with generic basis functions","title":"Optimal control with generic basis functions","text":"# Apply input trajectory to the actual system.\ny_sys = Array{Float64}(undef, n_y, H)\nx_sys = Array{Float64}(undef, n_x, H)\nx_sys[:, 1] = x_training[:, end]\nu_sys = [u_opt 0]\nfor t in 1:H\n    if t >= 2\n        x_sys[:, t] = f_true(x_sys[:, t-1], u_sys[:, t-1]) + rand(mvn_v_true, 1)\n    end\n    y_sys[:, t] = g_true(x_sys[:, t], u_sys[:, t]) + rand(mvn_e_true, 1)\nend\n\n# Plot predictions.\nplot_predictions(y_opt, y_sys; plot_percentiles=false, y_min=y_min, y_max=y_max)","category":"page"},{"location":"list_of_functions/#List-of-fuctions","page":"List of fuctions","title":"List of fuctions","text":"","category":"section"},{"location":"list_of_functions/","page":"List of fuctions","title":"List of fuctions","text":"particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi::Function, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R; x_prim=nothing)\nsolve_PG_OCP(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, active_constraints=nothing, opts=nothing, print_progress=true)\nsolve_PG_OCP_greedy_guarantees(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag, β; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, opts=nothing, print_progress=true)\ntest_prediction(PG_samples::Vector{PG_sample}, phi::Function, g, R, k_n, u_test, y_test)\nplot_predictions(y_pred, y_test; plot_percentiles=false, y_min=nothing, y_max=nothing)\nplot_autocorrelation(PG_samples::Vector{PG_sample}; max_lag=0)\nepsilon(s::Int64, K::Int64, β::Float64)\nMNIW_sample(Phi, Psi, Sigma, V, Lambda_Q, ell_Q, T)\nsystematic_resampling(W, N)","category":"page"},{"location":"list_of_functions/#PGopt.particle_Gibbs-Tuple{Any, Any, Any, Any, Any, Any, Function, Vararg{Any, 8}}","page":"List of fuctions","title":"PGopt.particle_Gibbs","text":"particle_Gibbs(u_training, y_training, K, K_b, k_d, N, phi::Function, Lambda_Q, ell_Q, Q_init, V, A_init, x_init_dist, g, R; x_prim=nothing)\n\nRun particle Gibbs sampler with ancestor sampling to obtain samples A Q x_T-1^1K from the joint parameter and state posterior distribution p(A Q x_T-1 mid mathbbD=u_T-1 y_T-1).\n\nArguments\n\nu_training: training input trajectory\ny_training: training output trajectory\nK: number of models/scenarios to be sampled\nK_b: length of the burn in period\nk_d: number of models/scenarios to be skipped to decrease correlation (thinning)\nN: number of particles\nphi: basis functions\nLambda_Q: scale matrix of IW prior on Q\nell_Q: degrees of freedom of IW prior on Q\nQ_init: initial value of Q\nV: left covariance matrix of MN prior on A\nA_init: initial value of A\nx_init_dist: distribution of the initial state\ng: observation function\nR: variance of zero-mean Gaussian measurement noise\nx_prim: prespecified trajectory for the first iteration\n\nThis function is based on the papers\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nF. Lindsten, T. B. Schön, and M. Jordan, “Ancestor sampling for particle Gibbs,” Advances in Neural Information Processing Systems, vol. 25, 2012.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.solve_PG_OCP-Tuple{Vector{PG_sample}, Function, Vararg{Any, 7}}","page":"List of fuctions","title":"PGopt.solve_PG_OCP","text":"solve_PG_OCP(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, active_constraints=nothing, opts=nothing, print_progress=true)\n\nSolve the optimal control problem of the following form:\n\nmin sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t\n\nsubject to:\n\nbeginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned\n\nNote that the output constraints imply the measurement function y_t^k = x_t 1n_y^k. Further note that the states of the individual models (x^1K) are combined in the vector x_vec of dimension K * n_x.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\nR: variance of zero-mean Gaussian measurement noise - only used if e_vec is not passed\nH: horizon of the OCP\nu_min: array of dimension 1 x 1 or 1 x H containing the minimum control input for all timesteps\nu_max: array of dimension 1 x 1 or 1 x H containing the maximum control input for all timesteps\ny_min: array of dimension 1 x 1 or 1 x H containing the minimum system output for all timesteps\ny_max: array of dimension 1 x 1 or 1 x H containing the maximum system output for all timesteps\nR_cost_diag: parameter of the diagonal quadratic cost function\nx_vec_0: vector with K n_x elements containing the initial state of all models - if not provided, the initial states are sampled based on the PGS samples\nv_vec: array of dimension n_x x H x K that contains the process noise for all models and all timesteps - if not provided, the noise is sampled based on the PGS samples\ne_vec: array of dimension n_y x H x K that contains the measurement noise for all models and all timesteps - if not provided, the noise is sampled based on the provided R\nu_init: initial guess for the optimal trajectory\nK_pre_solve: if K_pre_solve > 0, an initial guess for the optimal trajectory is obtained by solving the OCP with only K_pre_solve < K models\nactive_constraints: vector containing the indices of the models, for which the output constraints are active - if not provided, the output constraints are considered for all models\nopts: SolverOptions struct containing options of the solver\nprint_progress: if set to true, the progress is printed\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.solve_PG_OCP_greedy_guarantees-Tuple{Vector{PG_sample}, Function, Vararg{Any, 8}}","page":"List of fuctions","title":"PGopt.solve_PG_OCP_greedy_guarantees","text":"solve_PG_OCP_greedy_guarantees(PG_samples::Vector{PG_sample}, phi::Function, R, H, u_min, u_max, y_min, y_max, R_cost_diag, β; x_vec_0=nothing, v_vec=nothing, e_vec=nothing, u_init=nothing, K_pre_solve=0, opts=nothing, print_progress=true)\n\nSolve the following optimal control problem and determine a support sub-sample with cardinality s via a greedy constraint removal.  Based on the cardinality s, a bound on the probability that the incurred cost exceeds the worst-case cost or that the constraints are violated when the input trajectory u_0H is applied to the unknown system is calculated (i.e., 1-epsilon is determined).\n\nmin sum_t=0^H frac12  u_t  operatornamediag(R_mathrmcost) u_t\n\nsubject to:\n\nbeginaligned\nforall k forall t \nx_t+1^k = f_theta^k(x_t^k u_t) + v_t^k \nx_t 1n_y^k geq y_mathrmmin t - e_t^k \nx_t 1n_y^k leq y_mathrmmax t - e_t^k \nu_t geq u_mathrmmin t \nu_t leq u_mathrmmax t\nendaligned\n\nNote that the output constraints imply the measurement function y_t^k = x_t 1n_y^k. Further note that the states of the individual models (x^1K) are combined in the vector x_vec of dimension K * n_x.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\nR: variance of zero-mean Gaussian measurement noise - only used if e_vec is not passed\nH: horizon of the OCP\nu_min: array of dimension 1 x 1 or 1 x H containing the minimum control input for all timesteps\nu_max: array of dimension 1 x 1 or 1 x H containing the maximum control input for all timesteps\ny_min: array of dimension 1 x 1 or 1 x H containing the minimum system output for all timesteps\ny_max: array of dimension 1 x 1 or 1 x H containing the maximum system output for all timesteps\nR_cost_diag: parameter of the diagonal quadratic cost function\nβ: confidence parameter\nx_vec_0: vector with K*n_x elements containing the initial state of all models - if not provided, the initial states are sampled based on the PGS samples\nv_vec: array of dimension n_x x H x K that contains the process noise for all models and all timesteps - if not provided, the noise is sampled based on the PGS samples\ne_vec: array of dimension n_y x H x K that contains the measurement noise for all models and all timesteps - if not provided, the noise is sampled based on the provided R\nu_init: initial guess for the optimal trajectory\nK_pre_solve: if K_pre_solve > 0, an initial guess for the optimal trajectory is obtained by solving the OCP with only K_pre_solve < K models\nopts: SolverOptions struct containing options of the solver\nprint_progress: if set to true, the progress is printed\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.test_prediction-Tuple{Vector{PG_sample}, Function, Vararg{Any, 5}}","page":"List of fuctions","title":"PGopt.test_prediction","text":"test_prediction(PG_samples::Vector{PG_sample}, phi::Function, g, R, k_n, u_test, y_test)\n\nSimulate the PGS samples forward in time and compare the predictions to the test data.\n\nArguments\n\nPG_samples: PG samples\nphi: basis functions\ng: observation function\nR: variance of zero-mean Gaussian measurement noise\nk_n: each model is simulated k_n times\nu_test: test input\ny_test: test output\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.plot_predictions-Tuple{Any, Any}","page":"List of fuctions","title":"PGopt.plot_predictions","text":"plot_predictions(y_pred, y_test; plot_percentiles=false, y_min=nothing, y_max=nothing)\n\nPlot the predictions and the test data.\n\nArguments\n\ny_pred: matrix containing the output predictions\ny_test: test output trajectory\nplot_percentiles: if set to true, percentiles are plotted\ny_min: min output to be plotted as constraint\ny_max: max output to be plotted as constraint\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.plot_autocorrelation-Tuple{Vector{PG_sample}}","page":"List of fuctions","title":"PGopt.plot_autocorrelation","text":"plot_autocorrelation(PG_samples::Vector{PG_sample}; max_lag=0)\n\nPlot the autocorrelation function (ACF) of the PG samples. This might be helpful when adjusting the thinning parameter k_d.\n\nArguments\n\nPG_samples: PG samples\nmax_lag: maximum lag at which to calculate the ACF\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.epsilon-Tuple{Int64, Int64, Float64}","page":"List of fuctions","title":"PGopt.epsilon","text":"epsilon(s::Int64, K::Int64, β::Float64)\n\nDetermine the parameter epsilon. 1-epsilon corresponds to a bound on the probability that the incurred cost exceeds the worst-case cost or that the constraints are violated when the input trajectory u_0H is applied to the unknown system. epsilon is the unique solution over the interval (01) of the polynomial equation in the v variable:\n\nbinomKs(1-v)^K-s-fracbetaKsum_m=s^K-1binomms(1-v)^m-s=0.\n\nArguments\n\ns: cardinality of the support sub-sample \nK: number of scenarios\nβ: confidence parameter\n\nThis function is based on the paper\n\nS. Garatti and M. C. Campi, “Risk and complexity in scenario optimization,” Mathematical Programming, vol. 191, no. 1, pp. 243–279, 2022.\n\nand the code provided in the appendix.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.MNIW_sample-NTuple{7, Any}","page":"List of fuctions","title":"PGopt.MNIW_sample","text":"MNIW_sample(Phi, Psi, Sigma, V, Lambda_Q, ell_Q, T)\n\nSample new model parameters A Q from the conditional distribution p(A Q  x_T-1), which is a matrix normal inverse Wishart (MNIW) distribution.\n\nArguments\n\nPhi: statistic; see paper below for definition\nPsi: statistic; see paper below for definition\nSigma: statistic; see paper below for definition\nV: left covariance matrix of MN prior on A\nLambda_Q: scale matrix of IW prior on Q\nell_Q: degrees of freedom of IW prior on Q\nT: length of the training trajectory\n\nThis function is based on the paper\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"list_of_functions/#PGopt.systematic_resampling-Tuple{Any, Any}","page":"List of fuctions","title":"PGopt.systematic_resampling","text":"systematic_resampling(W, N)\n\nSample N indices according to the weights W.\n\nArguments\n\nW: vector containing the weights of the particles\nN: number of indices to be sampled\n\nThis function is based on the paper\n\nA. Svensson and T. B. Schön, “A flexible state–space model for learning nonlinear dynamical systems,” Automatica, vol. 80, pp. 189–199, 2017.\n\nand the code provided in the supplementary material.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"If you found this software useful for your research, consider citing us.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"@article{PMCMC_OCP_arXiv_2023,\n      title={Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States},\n      author={Lefringhausen, Robert and Srithasan, Supitsana and Lederer, Armin and Hirche, Sandra},\n      journal={arXiv preprint arXiv:2303.17963},\n      year={2023}\n}","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"PGopt is a software for determining optimal input trajectories with probabilistic performance and constraint satisfaction guarantees for unknown systems with latent states based on input-output measurements. In order to quantify uncertainties, which is crucial for deriving formal guarantees, a Bayesian approach is employed and a prior over the unknown dynamics and the system trajectory is formulated in state-space representation. Since for practical applicability, the prior must be updated based on input-output measurements, but the corresponding posterior distribution is analytically intractable, particle Gibbs (PG) sampling is utilized to draw samples from this distribution. Based on these samples, a scenario optimal control problem (OCP) is formulated and probabilistic performance and constraint satisfaction guarantees are inferred via a greedy constraint removal.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The approach is explained in the paper \"Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States\", available as a preprint on arXiv.","category":"page"},{"location":"#Versions","page":"Introduction","title":"Versions","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This document describes the Julia implementation of PGopt, which does not require proprietary software. The open-source solver Altro is used for the optimization. The results presented in the paper were generated with this version, and the software reproduces these results exactly.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Please note that this version has several limitations: only cost functions of the form J_H=sumnolimits_t=0^H frac12 u_t R u_t, measurement functions of the form y=x_1n_y, and output constraints of the form y_mathrmmin leq y leq y_mathrmmax are supported.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Besides the Julia implementation, there is also a MATLAB implementation, which is more general and allows arbitrary cost functions J_H(u_1Hx_1Hy_1H), measurement functions y=g(xu), and constraints h(u_1Hx_1Hy_1H). Further information can be found here.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"PGopt can be installed using the Julia package manager. Start a Pkg REPL (press ] in a Julia REPL), and install PGopt via","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add https://github.com/TUM-ITR/PGopt:Julia","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Alternatively, to inspect the source code more easily, download the source code from GitHub. Navigate to the folder PGopt/Julia, start a Pkg REPL (press ] in a Julia REPL), and install the dependencies via","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg>activate . \npkg>instantiate","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"You can then execute the scripts autocorrelation.jl, PG_OCP_known_basis_functions.jl, or PG_generic_basis_functions.jl in the folder PGopt/Julia/examples.","category":"page"}]
}
